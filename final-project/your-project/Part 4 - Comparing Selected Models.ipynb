{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from nltk.metrics import ConfusionMatrix\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# My own functions\n",
    "from NLP_Functions import find_features, make_matrix, clean_up, tokenize, stem_and_lemmatize, remove_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "comments = pd.read_csv('Datasets/games_comments_cleaned.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Platform</th>\n",
       "      <th>Userscore</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Super Smash Bros. for Wii U</td>\n",
       "      <td>WiiU</td>\n",
       "      <td>9</td>\n",
       "      <td>This game is amazing, improves in every aspect...</td>\n",
       "      <td>Zin_49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Zero Escape: Zero Time Dilemma</td>\n",
       "      <td>3DS</td>\n",
       "      <td>8</td>\n",
       "      <td>A bunch of bootleg robotic versions of the mai...</td>\n",
       "      <td>Techbane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Metro: Last Light</td>\n",
       "      <td>PC</td>\n",
       "      <td>6</td>\n",
       "      <td>I liked the original this is some innovation b...</td>\n",
       "      <td>gstiker5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PlanetSide 2</td>\n",
       "      <td>PC</td>\n",
       "      <td>5</td>\n",
       "      <td>Planetside 2  has battles of epic scale and de...</td>\n",
       "      <td>NasseSeta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Diablo III</td>\n",
       "      <td>PC</td>\n",
       "      <td>1</td>\n",
       "      <td>Game looks good and overall i like the graphic...</td>\n",
       "      <td>JamesLFranco</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>Halo: Reach</td>\n",
       "      <td>Xbox360</td>\n",
       "      <td>9</td>\n",
       "      <td>Halo is far superior to it's competition, alwa...</td>\n",
       "      <td>IcyGames</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>Uncharted 4: A Thief's End</td>\n",
       "      <td>PlayStation4</td>\n",
       "      <td>10</td>\n",
       "      <td>A Great Game even includes a Crash Bandicoot M...</td>\n",
       "      <td>Daxterman20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14997</th>\n",
       "      <td>Cuphead</td>\n",
       "      <td>XboxOne</td>\n",
       "      <td>10</td>\n",
       "      <td>Easily one of the best games of the year with ...</td>\n",
       "      <td>simsy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14998</th>\n",
       "      <td>The Elder Scrolls III: Morrowind</td>\n",
       "      <td>PC</td>\n",
       "      <td>8</td>\n",
       "      <td>This game would easily be 10 out of 10. I've p...</td>\n",
       "      <td>cimerians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14999</th>\n",
       "      <td>Thief: Deadly Shadows</td>\n",
       "      <td>PC</td>\n",
       "      <td>0</td>\n",
       "      <td>Crashes, crashes, and more crashes. I finishe...</td>\n",
       "      <td>Demo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Title      Platform  Userscore  \\\n",
       "0           Super Smash Bros. for Wii U          WiiU          9   \n",
       "1        Zero Escape: Zero Time Dilemma           3DS          8   \n",
       "2                     Metro: Last Light            PC          6   \n",
       "3                          PlanetSide 2            PC          5   \n",
       "4                            Diablo III            PC          1   \n",
       "...                                 ...           ...        ...   \n",
       "14995                       Halo: Reach       Xbox360          9   \n",
       "14996        Uncharted 4: A Thief's End  PlayStation4         10   \n",
       "14997                           Cuphead       XboxOne         10   \n",
       "14998  The Elder Scrolls III: Morrowind            PC          8   \n",
       "14999             Thief: Deadly Shadows            PC          0   \n",
       "\n",
       "                                                 Comment      Username  \n",
       "0      This game is amazing, improves in every aspect...        Zin_49  \n",
       "1      A bunch of bootleg robotic versions of the mai...      Techbane  \n",
       "2      I liked the original this is some innovation b...      gstiker5  \n",
       "3      Planetside 2  has battles of epic scale and de...     NasseSeta  \n",
       "4      Game looks good and overall i like the graphic...  JamesLFranco  \n",
       "...                                                  ...           ...  \n",
       "14995  Halo is far superior to it's competition, alwa...      IcyGames  \n",
       "14996  A Great Game even includes a Crash Bandicoot M...   Daxterman20  \n",
       "14997  Easily one of the best games of the year with ...         simsy  \n",
       "14998  This game would easily be 10 out of 10. I've p...     cimerians  \n",
       "14999   Crashes, crashes, and more crashes. I finishe...          Demo  \n",
       "\n",
       "[15000 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a 15k sample this time, to see if things are a little better over a 5k sample\n",
    "df = comments.sample(15000, random_state = 18)  # random_state for reproducibility\n",
    "\n",
    "# Resetting the index\n",
    "df.reset_index(drop = True, inplace = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the 'Target' (label) column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be testing 2 different label classifications, one of my own (Target) and one suggested from the NPS system (Target_NPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive    8724\n",
      "Negative    6276\n",
      "Name: Target, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Positive    8724\n",
       "Negative    3812\n",
       "Neutral     2464\n",
       "Name: Target_NPS, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the 'Target' column\n",
    "## 9-10 positive\n",
    "## 8 and below negative\n",
    "df['Target'] = np.where((df['Userscore'] <= 8), 'Negative', 'Positive')  # Target_3 from previous notebook\n",
    "\n",
    "\n",
    "# Creating the 'Target_NPS' column\n",
    "## NPS Scale - https://en.wikipedia.org/wiki/Net_Promoter\n",
    "## 9-10 = positive\n",
    "## 7-8 = neutral\n",
    "## 0-6 = negative\n",
    "df['Target_NPS'] = np.where((df['Userscore'] <= 6), 'Negative', 'Positive')\n",
    "df['Target_NPS'] = np.where(((df['Userscore'] >= 7) & (df['Userscore'] <= 8)), 'Neutral', df['Target_NPS'])\n",
    "\n",
    "\n",
    "# Checking the different proportion of values\n",
    "## unsure if I should balance these or not\n",
    "print(df['Target'].value_counts())\n",
    "df['Target_NPS'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the 'Comments'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the 'Comments_Processed' column\n",
    "df['Comments_Processed'] = df['Comment'].apply(lambda x: stem_and_lemmatize(remove_stopwords(tokenize(clean_up(x)))))\n",
    "## wonder if there's a more efficient way of doing this. It took a few minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Platform</th>\n",
       "      <th>Userscore</th>\n",
       "      <th>Comment</th>\n",
       "      <th>Username</th>\n",
       "      <th>Target</th>\n",
       "      <th>Target_NPS</th>\n",
       "      <th>Comments_Processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Super Smash Bros. for Wii U</td>\n",
       "      <td>WiiU</td>\n",
       "      <td>9</td>\n",
       "      <td>This game is amazing, improves in every aspect...</td>\n",
       "      <td>Zin_49</td>\n",
       "      <td>Positive</td>\n",
       "      <td>Positive</td>\n",
       "      <td>[game, amaz, improv, everi, aspect, previou, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Zero Escape: Zero Time Dilemma</td>\n",
       "      <td>3DS</td>\n",
       "      <td>8</td>\n",
       "      <td>A bunch of bootleg robotic versions of the mai...</td>\n",
       "      <td>Techbane</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>[bunch, bootleg, robot, version, main, charact...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Metro: Last Light</td>\n",
       "      <td>PC</td>\n",
       "      <td>6</td>\n",
       "      <td>I liked the original this is some innovation b...</td>\n",
       "      <td>gstiker5</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[like, origin, innov, silent, main, charact, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PlanetSide 2</td>\n",
       "      <td>PC</td>\n",
       "      <td>5</td>\n",
       "      <td>Planetside 2  has battles of epic scale and de...</td>\n",
       "      <td>NasseSeta</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[planetsid, battl, epic, scale, decent, gfx, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Diablo III</td>\n",
       "      <td>PC</td>\n",
       "      <td>1</td>\n",
       "      <td>Game looks good and overall i like the graphic...</td>\n",
       "      <td>JamesLFranco</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "      <td>[game, look, good, overal, like, graphic, game...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Title Platform  Userscore  \\\n",
       "0     Super Smash Bros. for Wii U     WiiU          9   \n",
       "1  Zero Escape: Zero Time Dilemma      3DS          8   \n",
       "2               Metro: Last Light       PC          6   \n",
       "3                    PlanetSide 2       PC          5   \n",
       "4                      Diablo III       PC          1   \n",
       "\n",
       "                                             Comment      Username    Target  \\\n",
       "0  This game is amazing, improves in every aspect...        Zin_49  Positive   \n",
       "1  A bunch of bootleg robotic versions of the mai...      Techbane  Negative   \n",
       "2  I liked the original this is some innovation b...      gstiker5  Negative   \n",
       "3  Planetside 2  has battles of epic scale and de...     NasseSeta  Negative   \n",
       "4  Game looks good and overall i like the graphic...  JamesLFranco  Negative   \n",
       "\n",
       "  Target_NPS                                 Comments_Processed  \n",
       "0   Positive  [game, amaz, improv, everi, aspect, previou, s...  \n",
       "1    Neutral  [bunch, bootleg, robot, version, main, charact...  \n",
       "2   Negative  [like, origin, innov, silent, main, charact, e...  \n",
       "3   Negative  [planetsid, battl, epic, scale, decent, gfx, g...  \n",
       "4   Negative  [game, look, good, overal, like, graphic, game...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the basic variables for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''INITIALIZING EVERYTHING TO BEGIN TESTING WITH DIFFERENT MODELS'''\n",
    "# Creating the bag of words\n",
    "bow = [word for lst in df['Comments_Processed'] for word in lst]\n",
    "fdist = FreqDist(bow)\n",
    "\n",
    "# Getting just the 10k most common words\n",
    "most_common = fdist.most_common(10000)  # WILL TRY CHANGING THIS A BIT TO SEE THE ACCURACY IMPACTS\n",
    "\n",
    "'''TARGET'''\n",
    "# Building the features and making the matrix\n",
    "matrix = make_matrix(df['Comment'], df['Target'], most_common)\n",
    "\n",
    "# Defining the size to use for the training and testing\n",
    "size = int(len(matrix) * 0.20)\n",
    "\n",
    "# Training with 80% of the data and testing against the remaining 20%\n",
    "training_set = matrix[size:]\n",
    "testing_set = matrix[:size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''TARGET_NPS'''\n",
    "# Building the features and making the matrix\n",
    "matrix_NPS = make_matrix(df['Comment'], df['Target_NPS'], most_common)\n",
    "\n",
    "# Defining the size to use for the training and testing\n",
    "size_NPS = int(len(matrix_NPS) * 0.20)\n",
    "\n",
    "# Training with 80% of the data and testing against the remaining 20%\n",
    "training_set_NPS = matrix_NPS[size_NPS:]\n",
    "testing_set_NPS = matrix_NPS[:size_NPS]\n",
    "\n",
    "\n",
    "'''\n",
    "# Initializing and training the model\n",
    "classifier_1 = nltk.NaiveBayesClassifier.train(training_set_1)\n",
    "\n",
    "# Showing the top 15 most informative features\n",
    "classifier_1.show_most_informative_features(15)\n",
    "\n",
    "# Printing the model's accuracy\n",
    "print('\\n', 'Original NLTK NB accuracy (Target):', \n",
    "      str(round(nltk.classify.accuracy(classifier_1, testing_set_1) * 100, 2)) + '%')'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial NB Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with 'Target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB accuracy (Target): 73.17%\n"
     ]
    }
   ],
   "source": [
    "# TESTING WITH TARGET\n",
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)\n",
    "\n",
    "print('MultinomialNB accuracy (Target):', \n",
    "      str(round(nltk.classify.accuracy(MNB_classifier, testing_set) * 100, 2)) + '%')\n",
    "# default: 73.17% - seems ok? (before with 5k sample it was 68.3% or so)\n",
    "# alpha = 2: 69.4% - NOT TESTED HERE YET\n",
    "\n",
    "# I will probably use this one I guess, if nothing better comes along"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix for 'Target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025\n",
      "975\n"
     ]
    }
   ],
   "source": [
    "# Predictions of the testing_set with the Target column\n",
    "pred_list_MNB = [MNB_classifier.classify(testing_set[i][0]) for i in range(len(testing_set))]\n",
    "print(pred_list_MNB.count('Positive'))  # 2025\n",
    "print(pred_list_MNB.count('Negative'))  # 975"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         |    N    P |\n",
      "         |    e    o |\n",
      "         |    g    s |\n",
      "         |    a    i |\n",
      "         |    t    t |\n",
      "         |    i    i |\n",
      "         |    v    v |\n",
      "         |    e    e |\n",
      "---------+-----------+\n",
      "Negative | <711> 541 |\n",
      "Positive |  264<1484>|\n",
      "---------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "TP + TN: 2195 Counter({'Positive': 1484, 'Negative': 711})\n",
      "FP + FN: 805 Counter({'Positive': 541, 'Negative': 264})\n"
     ]
    }
   ],
   "source": [
    "# Unpacking the references (Positive or Negative) of the testing set to use for the confusion matrix\n",
    "ref = [testing_set[i][1] for i in range(len(testing_set))]  # pos: 1748 | neg: 1258\n",
    "\n",
    "# List of predictions ran above\n",
    "tagged = pred_list_MNB\n",
    "\n",
    "# The actual confusion matrix\n",
    "cm = ConfusionMatrix(ref, tagged)\n",
    "\n",
    "print(cm)\n",
    "cm\n",
    "\n",
    "labels = set('Positive Negative'.split())\n",
    "\n",
    "true_positives = Counter()\n",
    "false_negatives = Counter()\n",
    "false_positives = Counter()\n",
    "\n",
    "for i in labels:\n",
    "    for j in labels:\n",
    "        if i == j:\n",
    "            true_positives[i] += cm[i, j]\n",
    "        else:\n",
    "            false_negatives[i] += cm[i, j]\n",
    "            false_positives[j] += cm[i, j]\n",
    "\n",
    "print('TP + TN:', sum(true_positives.values()), true_positives)\n",
    "print('FP + FN:', sum(false_positives.values()), false_positives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with 'Target_NPS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB accuracy (Target_NPS): 67.93%\n"
     ]
    }
   ],
   "source": [
    "# TESTING WITH TARGET_NPS\n",
    "MNB_classifier_NPS = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier_NPS.train(training_set_NPS)\n",
    "\n",
    "print('MultinomialNB accuracy (Target_NPS):', \n",
    "      str(round(nltk.classify.accuracy(MNB_classifier_NPS, testing_set_NPS) * 100, 2)) + '%')\n",
    "# default: 67.93%  (before with 5k sample it was 68.3%)\n",
    "# alpha = 2: 69.4% - NOT TESTED HERE YET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix for 'Target_NPS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1946\n",
      "649\n",
      "405\n"
     ]
    }
   ],
   "source": [
    "# Predictions of the testing_set_NPS with the Target_NPS column\n",
    "pred_list_MNB_NPS = [MNB_classifier_NPS.classify(testing_set_NPS[i][0]) for i in range(len(testing_set_NPS))]\n",
    "print(pred_list_MNB_NPS.count('Positive'))  # 1946\n",
    "print(pred_list_MNB_NPS.count('Negative'))  # 649\n",
    "print(pred_list_MNB_NPS.count('Neutral'))  # 405"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         |    N         P |\n",
      "         |    e    N    o |\n",
      "         |    g    e    s |\n",
      "         |    a    u    i |\n",
      "         |    t    t    t |\n",
      "         |    i    r    i |\n",
      "         |    v    a    v |\n",
      "         |    e    l    e |\n",
      "---------+----------------+\n",
      "Negative | <545> 203  504 |\n",
      " Neutral |    .   <.>   . |\n",
      "Positive |  104  202<1442>|\n",
      "---------+----------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "TP + TN: 1987 Counter({'Positive': 1442, 'Negative': 545, 'Neutral': 0})\n",
      "FP + FN: 1013 Counter({'Positive': 504, 'Neutral': 405, 'Negative': 104})\n"
     ]
    }
   ],
   "source": [
    "# Unpacking the references (Positive, Negative, Neutral) of the testing_set_NPS to use for the confusion matrix\n",
    "ref_NPS = [testing_set_NPS[i][1] for i in range(len(testing_set_NPS))]  # pos: 1748 | neg: 782 | neut: 470\n",
    "\n",
    "# List of predictions ran above\n",
    "tagged = pred_list_MNB_NPS\n",
    "\n",
    "# The actual confusion matrix\n",
    "cm = ConfusionMatrix(ref, tagged)\n",
    "\n",
    "print(cm)\n",
    "cm\n",
    "\n",
    "labels = set('Positive Negative Neutral'.split())\n",
    "\n",
    "true_positives = Counter()\n",
    "false_negatives = Counter()\n",
    "false_positives = Counter()\n",
    "\n",
    "for i in labels:\n",
    "    for j in labels:\n",
    "        if i == j:\n",
    "            true_positives[i] += cm[i, j]\n",
    "        else:\n",
    "            false_negatives[i] += cm[i, j]\n",
    "            false_positives[j] += cm[i, j]\n",
    "\n",
    "# Since I have 3 labels here the results are seeming a bit strange? Where is that 405 neutral from? Not in the CM\n",
    "print('TP + TN:', sum(true_positives.values()), true_positives)\n",
    "print('FP + FN:', sum(false_positives.values()), false_positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNB_classifier prediction: Negative\n",
      "MNB_classifier_NPS prediction: Negative\n"
     ]
    }
   ],
   "source": [
    "# Prediction of the infamous sentence the original NLTK NB never got right\n",
    "bad_sentence = find_features('This game sucks so much. I hate it a lot. This is complete garbage', most_common)\n",
    "\n",
    "print('MNB_classifier prediction:', MNB_classifier.classify(bad_sentence))\n",
    "print('MNB_classifier_NPS prediction:', MNB_classifier_NPS.classify(bad_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with 'Target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mattymrc/.pyenv/versions/3.7.5/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression accuracy (Target): 79.0%\n"
     ]
    }
   ],
   "source": [
    "# TESTING WITH TARGET\n",
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression(solver = 'saga', n_jobs = -1))#, max_iter = 500))\n",
    "LogisticRegression_classifier.train(training_set)\n",
    "# saga solver = 78.7% w/ 5k most_common\n",
    "# saga solver = 79% w/ 10k most_common\n",
    "# solver = 'saga', n_jobs = -1, max_iter = 500 -> 78%, still didn't converge\n",
    "\n",
    "print('Logistic Regression accuracy (Target):', \n",
    "      str(round(nltk.classify.accuracy(LogisticRegression_classifier, testing_set) * 100, 2)) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix for 'Target'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1802\n",
      "1198\n"
     ]
    }
   ],
   "source": [
    "# Predictions of the testing_set with the Target column\n",
    "pred_list_LR = [LogisticRegression_classifier.classify(testing_set[i][0]) for i in range(len(testing_set))]\n",
    "print(pred_list_LR.count('Positive'))  # 1809 w/ remove_stopwords in wrong place | now: 1802\n",
    "print(pred_list_LR.count('Negative'))  # 1191 w/ remove_stopwords in wrong place | now: 11198"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         |    N    P |\n",
      "         |    e    o |\n",
      "         |    g    s |\n",
      "         |    a    i |\n",
      "         |    t    t |\n",
      "         |    i    i |\n",
      "         |    v    v |\n",
      "         |    e    e |\n",
      "---------+-----------+\n",
      "Negative | <910> 342 |\n",
      "Positive |  288<1460>|\n",
      "---------+-----------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "TP + TN: 2370 Counter({'Positive': 1460, 'Negative': 910})\n",
      "FP + FN: 630 Counter({'Positive': 342, 'Negative': 288})\n"
     ]
    }
   ],
   "source": [
    "# Unpacking the references (Positive or Negative) of the testing set to use for the confusion matrix\n",
    "ref = [testing_set[i][1] for i in range(len(testing_set))]  # pos: 1748 | neg: 1258\n",
    "\n",
    "# List of predictions ran above\n",
    "tagged = pred_list_LR\n",
    "\n",
    "# The actual confusion matrix\n",
    "cm = ConfusionMatrix(ref, tagged)\n",
    "\n",
    "print(cm)\n",
    "cm\n",
    "\n",
    "labels = set('Positive Negative'.split())\n",
    "\n",
    "true_positives = Counter()\n",
    "false_negatives = Counter()\n",
    "false_positives = Counter()\n",
    "\n",
    "for i in labels:\n",
    "    for j in labels:\n",
    "        if i == j:\n",
    "            true_positives[i] += cm[i, j]\n",
    "        else:\n",
    "            false_negatives[i] += cm[i, j]\n",
    "            false_positives[j] += cm[i, j]\n",
    "\n",
    "# This seems to be the best model as the accuracy already predicted\n",
    "print('TP + TN:', sum(true_positives.values()), true_positives)\n",
    "print('FP + FN:', sum(false_positives.values()), false_positives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with 'Target_NPS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mattymrc/.pyenv/versions/3.7.5/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression accuracy (Target_NPS): 72.9%\n"
     ]
    }
   ],
   "source": [
    "# TESTING WITH TARGET_NPS\n",
    "LogisticRegression_classifier_NPS = SklearnClassifier(LogisticRegression(solver = 'saga'))\n",
    "LogisticRegression_classifier_NPS.train(training_set_NPS)\n",
    "# saga solver = 72.9%\n",
    "\n",
    "print('Logistic Regression accuracy (Target_NPS):', \n",
    "      str(round(nltk.classify.accuracy(LogisticRegression_classifier_NPS, testing_set_NPS) * 100, 2)) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix for 'Target_NPS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1855\n",
      "774\n",
      "371\n"
     ]
    }
   ],
   "source": [
    "# Predictions of the testing_set_NPS with the Target_NPS column\n",
    "pred_list_LR_NPS = [LogisticRegression_classifier_NPS.classify(testing_set_NPS[i][0]) \n",
    "                    for i in range(len(testing_set_NPS))]\n",
    "print(pred_list_LR_NPS.count('Positive'))  # 1855\n",
    "print(pred_list_LR_NPS.count('Negative'))  # 774\n",
    "print(pred_list_LR_NPS.count('Neutral'))  # 371"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         |    N         P |\n",
      "         |    e    N    o |\n",
      "         |    g    e    s |\n",
      "         |    a    u    i |\n",
      "         |    t    t    t |\n",
      "         |    i    r    i |\n",
      "         |    v    a    v |\n",
      "         |    e    l    e |\n",
      "---------+----------------+\n",
      "Negative | <664> 214  374 |\n",
      " Neutral |    .   <.>   . |\n",
      "Positive |  110  157<1481>|\n",
      "---------+----------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "TP + TN: 2145 Counter({'Positive': 1481, 'Negative': 664, 'Neutral': 0})\n",
      "FP + FN: 855 Counter({'Positive': 374, 'Neutral': 371, 'Negative': 110})\n"
     ]
    }
   ],
   "source": [
    "# Unpacking the references (Positive, Negative, Neutral) of the testing_set_NPS to use for the confusion matrix\n",
    "ref_NPS = [testing_set_NPS[i][1] for i in range(len(testing_set_NPS))]  # pos: 1748 | neg: 782 | neut: 470\n",
    "\n",
    "# List of predictions ran above\n",
    "tagged = pred_list_LR_NPS\n",
    "\n",
    "# The actual confusion matrix\n",
    "cm = ConfusionMatrix(ref, tagged)\n",
    "\n",
    "print(cm)\n",
    "cm\n",
    "\n",
    "labels = set('Positive Negative Neutral'.split())\n",
    "\n",
    "true_positives = Counter()\n",
    "false_negatives = Counter()\n",
    "false_positives = Counter()\n",
    "\n",
    "for i in labels:\n",
    "    for j in labels:\n",
    "        if i == j:\n",
    "            true_positives[i] += cm[i, j]\n",
    "        else:\n",
    "            false_negatives[i] += cm[i, j]\n",
    "            false_positives[j] += cm[i, j]\n",
    "\n",
    "print('TP + TN:', sum(true_positives.values()), true_positives)\n",
    "print('FP + FN:', sum(false_positives.values()), false_positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression_classifier prediction: Negative\n",
      "LogisticRegression_classifier_NPS prediction: Negative\n"
     ]
    }
   ],
   "source": [
    "# Prediction of the infamous sentence the original NLTK NB never got right\n",
    "print('LogisticRegression_classifier prediction:', LogisticRegression_classifier.classify(bad_sentence))\n",
    "print('LogisticRegression_classifier_NPS prediction:', LogisticRegression_classifier_NPS.classify(bad_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVC Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The normal SVC model actually gave me pretty good results in the previous notebook.\n",
    "\n",
    "However, it takes too long to compute, and it is even advised to use either Linear SVC or SGD in the documentation of SVC for larger datasets. That's why I am not testing it again with this bigger sample, and also why I won't be using it in the final 280k dataset, as it will take forever to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mattymrc/.pyenv/versions/3.7.5/lib/python3.7/site-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC_classifier accuracy (Target): 74.93%\n"
     ]
    }
   ],
   "source": [
    "LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier.train(training_set)\n",
    "# Default - 74.93%\n",
    "\n",
    "print('LinearSVC_classifier accuracy (Target):', \n",
    "      str(round(nltk.classify.accuracy(LinearSVC_classifier, testing_set) * 100, 2)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC_classifier_NPS accuracy (Target_NPS): 69.43%\n"
     ]
    }
   ],
   "source": [
    "LinearSVC_classifier_NPS = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier_NPS.train(training_set_NPS)\n",
    "# Default - 69.43%\n",
    "\n",
    "print('LinearSVC_classifier_NPS accuracy (Target_NPS):', \n",
    "      str(round(nltk.classify.accuracy(LinearSVC_classifier_NPS, testing_set_NPS) * 100, 2)) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC_classifier prediction: Negative\n",
      "LinearSVC_classifier_NPS prediction: Negative\n"
     ]
    }
   ],
   "source": [
    "# Prediction of the infamous sentence the original NLTK NB never got right\n",
    "print('LinearSVC_classifier prediction:', LinearSVC_classifier.classify(bad_sentence))\n",
    "print('LinearSVC_classifier_NPS prediction:', LinearSVC_classifier_NPS.classify(bad_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to Predict a Sentence's/Review's Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Positive\n",
      "Prediction: Positive\n"
     ]
    }
   ],
   "source": [
    "# This was improved upon in notebook 6\n",
    "def predictor(text):\n",
    "    prediction = find_features(text, most_common)\n",
    "    return print('Prediction:', LogisticRegression_classifier.classify(prediction))\n",
    "\n",
    "predictor('This game is amazing!')\n",
    "predictor('I love this game!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the 5k most_common words of this 15k sample DF\n",
    "save_most_common = open('10kmost_common_15ksample.pickle', 'wb')\n",
    "pickle.dump(most_common, save_most_common)\n",
    "save_most_common.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the trained LR algorithm\n",
    "save_classifier = open('Logistic_Regression_15k.pickle', 'wb')\n",
    "pickle.dump(LogisticRegression_classifier, save_classifier)\n",
    "save_classifier.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting this 15k sample DF to use later for plots and everything\n",
    "df.to_json('Datasets/comments_processed_15ksample.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing opening the pickled Logistic Regression Trained Model\n",
    "#classifier_f = open('naivebayes.pickle', 'rb')\n",
    "#classifier_og = pickle.load(classifier_f)\n",
    "#classifier_f.close()\n",
    "\n",
    "#classifier_og  # this one is the trained algo with the whole comments df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
